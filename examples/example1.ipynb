{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: single-trial GLM estimation in a rapid event-related design\n",
    "\n",
    "### Natural Scenes Dataset core experiment, subj01, nsd01 scan session\n",
    "\n",
    "---------------------\n",
    "\n",
    "##### GLMsingle is new tool that provides efficient, scalable, and accurate single-trial fMRI response estimates.\n",
    "\n",
    "The purpose of this Example 1 notebook is to guide the user through basic\n",
    "calls to GLMsingle, using a representative, small-scale test dataset (in\n",
    "this case, an example session from a rapid event-related visual fMRI\n",
    "dataset - the Natural Scenes Dataset core experiment).\n",
    "\n",
    "The goal is to examine the effect of GLMsingle on the reliability of\n",
    "single-trial fMRI response estimates. By default, the tool implements a\n",
    "set of optimizations that improve upon generic GLM approaches by: (1)\n",
    "identifying an optimal hemodynamic response function (HRF) at each voxel,\n",
    "(2) deriving a set of useful GLM nuisance regressors via \"GLMdenoise\" and\n",
    "picking an optimal number to include in the final GLM, and (3) applying a\n",
    "custom amount of ridge regularization at each voxel using an efficient\n",
    "technique called \"fracridge\". The output of GLMsingle are GLM betas\n",
    "reflecting the estimated percent signal change in each voxel in response\n",
    "to each experimental stimulus or condition being modeled.\n",
    "\n",
    "Beyond directly improving the reliability of neural responses to repeated\n",
    "stimuli, these optimized techniques for signal estimation can have a\n",
    "range of desirable downstream effects such as: improving cross-subject\n",
    "representational similarity within and between datasets; improving the\n",
    "single-image decodability of evoked neural patterns via MVPA; and,\n",
    "decreasing the correlation in spatial patterns observed at neighboring\n",
    "timepoints in analysis of fMRI GLM outputs. See our video presentation at\n",
    "V-VSS 2020 for a summary of these phenomena as observed in recent\n",
    "massive-scale fMRI datasets (the Natural Scenes Dataset and BOLD5000):\n",
    "https://www.youtube.com/watch?v=yb3Nn7Han8o\n",
    "\n",
    "**Example 1 contains a full walkthrough of the process of loading an\n",
    "example dataset and design matrix, estimating neural responses using\n",
    "GLMsingle, estimating the reliability of responses at each voxel, and\n",
    "comparing those achieved via GLMsingle to those achieved using a baseline\n",
    "GLM.** After loading and visualizing formatted fMRI time-series and their\n",
    "corresponding design matrices, we will describe the default behavior of\n",
    "GLMsingle and show how to modify hyperparameters if the user desires.\n",
    "Throughout the notebook we will highlight important metrics and outputs\n",
    "using figures, print statements, and comments.\n",
    "\n",
    "Users encountering bugs, unexpected outputs, or other issues regarding\n",
    "GLMsingle shouldn't hesitate to raise an issue on GitHub:\n",
    "https://github.com/kendrickkay/GLMsingle/issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import function libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join, exists, split\n",
    "import time\n",
    "import urllib.request\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glmsingle.glmsingle import GLM_single\n",
    "\n",
    "# note: the fracridge repository is also necessary to run this code\n",
    "# for example, you could do:\n",
    "#      git clone https://github.com/nrdg/fracridge.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths and download the example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to the directory to which GLMsingle was installed\n",
    "homedir = split(os.getcwd())[0]\n",
    "\n",
    "# create directory for saving data\n",
    "datadir = join(homedir,'examples','data')\n",
    "os.makedirs(datadir,exist_ok=True)\n",
    "\n",
    "# create directory for saving outputs from example 1\n",
    "outputdir = join(homedir,'examples','example1outputs')\n",
    "\n",
    "print(f'directory to save example dataset:\\n\\t{datadir}\\n')\n",
    "print(f'directory to save example1 outputs:\\n\\t{outputdir}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download example dataset from GLMsingle OSF repository\n",
    "# data comes from the NSD dataset (subj01, nsd01 scan session).\n",
    "# see: https://www.biorxiv.org/content/10.1101/2021.02.22.432340v1.full.pdf\n",
    "\n",
    "datafn = join(datadir,'nsdcoreexampledataset.mat')\n",
    "\n",
    "# to save time, we'll skip the download if the example dataset already exists on disk\n",
    "if not exists(datafn):\n",
    "    \n",
    "    print(f'Downloading example dataset and saving to:\\n{datafn}')\n",
    "    \n",
    "    dataurl = 'https://osf.io/k89b2/download'\n",
    "    \n",
    "    # download the .mat file to the specified directory\n",
    "    urllib.request.urlretrieve(dataurl, datafn)\n",
    "    \n",
    "# load struct containing example dataset\n",
    "X = sio.loadmat(datafn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize BOLD data, design matrices, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that will contain bold time-series and design matrices from each run\n",
    "data = []\n",
    "design = []\n",
    "\n",
    "# iterate through each run of data\n",
    "for r in range(len(X['data'][0])):\n",
    "    \n",
    "    # index into struct, append each run's timeseries data to list\n",
    "    data.append(X['data'][0,r])\n",
    "    \n",
    "    # convert each run design matrix from sparse array to full numpy array, append\n",
    "    design.append(scipy.sparse.csr_matrix.toarray(X['design'][0,r]))\n",
    "    \n",
    "# get shape of data volume (XYZ) for convenience\n",
    "xyz = data[0].shape[:3]\n",
    "xyzt = data[0].shape\n",
    "\n",
    "# get metadata about stimulus duration and TR\n",
    "stimdur = X['stimdur'][0][0]\n",
    "tr = X['tr'][0][0]\n",
    "\n",
    "# get visual ROI mask identifying occipital cortex\n",
    "roi = X['ROI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample data and design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data -> consists of several runs of 4D volume files (x,y,z,t)  where\n",
    "# (t)ime is the 4th dimention. in this example, data consists of only a\n",
    "# single slice and has been prepared with a TR = 1s\n",
    "\n",
    "# ROI -> manually defined region in the occipital cortex. it is a binary\n",
    "# matrix where (x,y,z) = 1 corresponds to the cortical area that responded\n",
    "# to visual stimuli used in the NSD project.\n",
    "\n",
    "# design -> each run has a corresponding design matrix where each column\n",
    "# describes a single condition (conditions are repeated across runs). each\n",
    "# design matrix is binary with 1 specfing the time (TR) when the stimulus\n",
    "# is presented on the screen.\n",
    "\n",
    "# in this NSD scan session, there are a total of 750 trials, in which a\n",
    "# total of 583 distinct images are shown. (thus, some images were presented\n",
    "# more than once.) in the design matrix plotted below, there are 583 predictor\n",
    "# columns/conditions, one per distinct image. notice that the ordering of \n",
    "# conditions is pseudo-randomized. note also that in some runs not all images \n",
    "# are shown; if a column is empty it simply means that this image is shown \n",
    "# in a different run within the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example slice from run 1\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data[0][:,:,0,0])\n",
    "plt.title('example slice from run 1',fontsize=16)\n",
    "plt.subplot(122)\n",
    "plt.imshow(data[11][:,:,0,0])\n",
    "plt.title('example slice from run 12',fontsize=16)\n",
    "\n",
    "# plot example design matrix from run 1\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(design[0],interpolation='none')\n",
    "plt.title('example design matrix from run 1',fontsize=16)\n",
    "plt.xlabel('conditions',fontsize=16)\n",
    "plt.ylabel('time (TR)',fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some relevant metadata\n",
    "print(f'There are {len(data)} runs in total\\n')\n",
    "print(f'N = {data[0].shape[3]} TRs per run\\n')\n",
    "print(f'The dimensions of the data for each run are: {data[0].shape}\\n')\n",
    "print(f'The stimulus duration is {stimdur} seconds\\n')\n",
    "print(f'XYZ dimensionality is: {data[0].shape[:3]} (one slice only in this example)\\n')\n",
    "print(f'Numeric precision of data is: {type(data[0][0,0,0,0])}\\n')\n",
    "print(f'There are {np.sum(roi)} voxels in the included visual ROI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GLMsingle with default parameters to estimate single-trial betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs and figures will be stored in a folder (you can specify its name\n",
    "# as the 5th output to GLMsingle). model estimates can be also\n",
    "# saved to the 'results' variable which is the only output of\n",
    "# GLMsingle.\n",
    "\n",
    "# optional parameters below can be assigned to a structure, i.e., opt =\n",
    "# dict('wantlibrary':1, 'wantglmdenoise':1); options are the 6th input to\n",
    "# GLMsingle.\n",
    "\n",
    "# there are many options that can be specified; here, we comment on the\n",
    "# main options that one might want to modify/set. defaults for the options\n",
    "# are indicated below.\n",
    "\n",
    "# wantlibrary = 1 -> fit HRF to each voxel \n",
    "# wantglmdenoise = 1 -> use GLMdenoise \n",
    "# wantfracridge = 1 -> use ridge regression to improve beta estimates \n",
    "# chunklen = 50000 -> is the number of voxels that we will\n",
    "#    process at the same time. for setups with lower memory, you may need to \n",
    "#    decrease this number.\n",
    "\n",
    "# wantmemoryoutputs is a logical vector [A B C D] indicating which of the\n",
    "#     four model types to return in the output <results>. the user must be\n",
    "#     careful with this, as large datasets can require a lot of RAM. if you\n",
    "#     do not request the various model types, they will be cleared from\n",
    "#     memory (but still potentially saved to disk). default: [0 0 0 1]\n",
    "#     which means return only the final type-D model.\n",
    "\n",
    "# wantfileoutputs is a logical vector [A B C D] indicating which of the\n",
    "#     four model types to save to disk (assuming that they are computed). A\n",
    "#     = 0/1 for saving the results of the ONOFF model, B = 0/1 for saving\n",
    "#     the results of the FITHRF model, C = 0/1 for saving the results of the\n",
    "#     FITHRF_GLMdenoise model, D = 0/1 for saving the results of the\n",
    "#     FITHRF_GLMdenoise_RR model. default: [1 1 1 1] which means save all\n",
    "#     computed results to disk.\n",
    "\n",
    "# numpcstotry (optional) is a non-negative integer indicating the maximum\n",
    "#     number of GLMdenoise PCs to enter into the model. default: 10.\n",
    "\n",
    "# fracs (optional) is a vector of fractions that are greater than 0\n",
    "#     and less than or equal to 1. we automatically sort in descending\n",
    "#     order and ensure the fractions are unique. these fractions indicate\n",
    "#     the regularization levels to evaluate using fractional ridge\n",
    "#     regression (fracridge) and cross-validation. default:\n",
    "#     fliplr(.05:.05:1). a special case is when <fracs> is specified as a\n",
    "#     single scalar value. in this case, cross-validation is NOT performed\n",
    "#     for the type-D model, and we instead blindly use the supplied\n",
    "#     fractional value for the type-D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for saving GLMsingle outputs\n",
    "outputdir_glmsingle = join(homedir,'examples','example1outputs','GLMsingle')\n",
    "\n",
    "opt = dict()\n",
    "\n",
    "# set important fields for completeness (but these would be enabled by default)\n",
    "opt['wantlibrary'] = 1\n",
    "opt['wantglmdenoise'] = 1\n",
    "opt['wantfracridge'] = 1\n",
    "\n",
    "# for the purpose of this example we will keep the relevant outputs in memory\n",
    "# and also save them to the disk\n",
    "opt['wantfileoutputs'] = [1,1,1,1]\n",
    "opt['wantmemoryoutputs'] = [1,1,1,1]\n",
    "\n",
    "# running python GLMsingle involves creating a GLM_single object\n",
    "# and then running the procedure using the .fit() routine\n",
    "glmsingle_obj = GLM_single(opt)\n",
    "\n",
    "# visualize all the hyperparameters\n",
    "pprint(glmsingle_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this example saves output files to the folder  \"example1outputs/GLMsingle\"\n",
    "# if these outputs don't already exist, we will perform the time-consuming call to GLMsingle;\n",
    "# otherwise, we will just load from disk.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if not exists(outputdir_glmsingle):\n",
    "\n",
    "    print(f'running GLMsingle...')\n",
    "    \n",
    "    # run GLMsingle\n",
    "    results_glmsingle = glmsingle_obj.fit(\n",
    "       design,\n",
    "       data,\n",
    "       stimdur,\n",
    "       tr,\n",
    "       outputdir=outputdir_glmsingle)\n",
    "    \n",
    "    # we assign outputs of GLMsingle to the \"results_glmsingle\" variable.\n",
    "    # note that results_glmsingle['typea'] contains GLM estimates from an ONOFF model,\n",
    "    # where all images are treated as the same condition. these estimates\n",
    "    # could be potentially used to find cortical areas that respond to\n",
    "    # visual stimuli. we want to compare beta weights between conditions\n",
    "    # therefore we are not going to include the ONOFF betas in any analyses of \n",
    "    # voxel reliability\n",
    "    \n",
    "else:\n",
    "    print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_glmsingle}')\n",
    "    \n",
    "    # load existing file outputs if they exist\n",
    "    results_glmsingle = dict()\n",
    "    results_glmsingle['typea'] = np.load(join(outputdir_glmsingle,'TYPEA_ONOFF.npy'),allow_pickle=True).item()\n",
    "    results_glmsingle['typeb'] = np.load(join(outputdir_glmsingle,'TYPEB_FITHRF.npy'),allow_pickle=True).item()\n",
    "    results_glmsingle['typec'] = np.load(join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.npy'),allow_pickle=True).item()\n",
    "    results_glmsingle['typed'] = np.load(join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npy'),allow_pickle=True).item()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\n",
    "    '\\telapsed time: ',\n",
    "    f'{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of important outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outputs of GLMsingle are formally documented in its\n",
    "# header. here, we highlight a few of the more important outputs:\n",
    "\n",
    "# R2 -> is model accuracy expressed in terms of R^2 (percentage).\n",
    "\n",
    "# betasmd -> is the full set of single-trial beta weights (X x Y x Z x\n",
    "# TRIALS). beta weights are arranged in chronological order.\n",
    "\n",
    "# HRFindex -> is the 1-index of the best fit HRF. HRFs can be recovered\n",
    "# with getcanonicalHRFlibrary(stimdur,tr)\n",
    "\n",
    "# FRACvalue -> is the fractional ridge regression regularization level\n",
    "# chosen for each voxel. values closer to 1 mean less regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a slice of brain showing GLMsingle outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to plot several outputs from the FIT_HRF_GLMdenoise_RR GLM,\n",
    "# which contains the full set of GLMsingle optimizations.\n",
    "\n",
    "# we will plot betas, R2, optimal HRF indices, and the voxel frac values\n",
    "plot_fields = ['betasmd','R2','HRFindex','FRACvalue']\n",
    "colormaps = ['RdBu_r','hot','jet','copper']\n",
    "clims = [[-5,5],[0,85],[0,20],[0,1]]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for i in range(len(plot_fields)):\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    \n",
    "    if i == 0:\n",
    "        # when plotting betas, for simplicity just average across all image presentations\n",
    "        # this will yield a summary of whether voxels tend to increase or decrease their \n",
    "        # activity in response to the experimental stimuli (similar to outputs from \n",
    "        # an ONOFF GLM)\n",
    "        plot_data = np.nanmean(np.squeeze(results_glmsingle['typed'][plot_fields[i]]),2)\n",
    "        titlestr = 'average GLM betas (750 stimuli)'\n",
    "    \n",
    "    else:\n",
    "        # plot all other voxel-wise metrics as outputted from GLMsingle\n",
    "        plot_data = np.squeeze(results_glmsingle['typed'][plot_fields[i]].reshape(xyz))\n",
    "        titlestr = plot_fields[i]\n",
    "    \n",
    "    plt.imshow(plot_data,cmap=colormaps[i],clim=clims[i])\n",
    "    plt.colorbar()\n",
    "    plt.title(titlestr)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a baseline GLM to compare with GLMsingle outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison purposes we are going to run a standard GLM\n",
    "# without HRF fitting, GLMdenoise, or ridge regression regularization. we\n",
    "# will compute the split-half reliability at each voxel using this baseline\n",
    "# GLM, and then assess whether reliability improves using the output betas\n",
    "# from GLMsingle. \n",
    "\n",
    "# output directory for baseline GLM\n",
    "outputdir_baseline = join(outputdir,'GLMbaseline')\n",
    "\n",
    "# we will run this baseline GLM by changing the default settings in GLMsingle \n",
    "# contained within the \"opt\" structure.\n",
    "opt = dict() \n",
    "\n",
    "# turn off optimizations \n",
    "opt['wantlibrary'] = 0 # switch off HRF fitting\n",
    "opt['wantglmdenoise'] = 0 # switch off GLMdenoise\n",
    "opt['wantfracridge'] = 0 # switch off ridge regression\n",
    "\n",
    "\n",
    "# for the purpose of this example we will keep the relevant outputs in memory\n",
    "# and also save them to the disk...\n",
    "# the first two indices are the ON-OFF GLM and the baseline single-trial GLM. \n",
    "# no need to save the third (+ GLMdenoise) and fourth (+ fracridge) outputs\n",
    "# since they will not even be computed\n",
    "opt['wantmemoryoutputs'] = [1,1,0,0] \n",
    "opt['wantfileoutputs'] = [1,1,0,0]\n",
    "\n",
    "# running python GLMsingle involves creating a GLM_single object\n",
    "# and then running the procedure using the .fit() routine\n",
    "glmbaseline_obj = GLM_single(opt)\n",
    "\n",
    "# visualize the hyperparameters, including the modified baseline opts\n",
    "pprint(glmbaseline_obj.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# if these outputs don't already exist, we will perform the call to\n",
    "# GLMsingle; otherwise, we will just load from disk.\n",
    "if not exists(outputdir_baseline):\n",
    "    \n",
    "    print(f'running GLMsingle...')\n",
    "\n",
    "    # run GLMsingle, fitting the baseline GLM\n",
    "    results_assumehrf = glmbaseline_obj.fit(\n",
    "       design,\n",
    "       data,\n",
    "       stimdur,\n",
    "       tr,\n",
    "       outputdir=outputdir_baseline)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f'loading existing GLMsingle outputs from directory:\\n\\t{outputdir_glmsingle}')\n",
    "    \n",
    "    results_assumehrf = dict()\n",
    "    results_assumehrf['typea'] = np.load(join(outputdir_baseline,'TYPEA_ONOFF.npy'),allow_pickle=True).item()\n",
    "    results_assumehrf['typeb'] = np.load(join(outputdir_baseline,'TYPEB_FITHRF.npy'),allow_pickle=True).item()\n",
    "    \n",
    "    # note that even though we are loading TYPEB_FITHRF betas, HRF fitting\n",
    "    # has been turned off and this struct field will thus contain the\n",
    "    # outputs of a GLM fit using the canonical HRF.\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\n",
    "    '\\telapsed time: ',\n",
    "    f'{time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary containing the GLM betas from the four different models we will compare.\n",
    "# note that the \"assume hrf\" betas come from the \"typeb\" field of our baseline GLM\n",
    "# (with HRF fitting turned off), and that the \"fit hrf\" betas also come from \n",
    "# the \"typeb\" field of the GLM that ran with all default GLMsingle routines\n",
    "# enabled\n",
    "\n",
    "models = dict()\n",
    "models['assumehrf'] = results_assumehrf['typeb']['betasmd'].reshape(xyz + (750,))\n",
    "models['fithrf'] = results_glmsingle['typeb']['betasmd']\n",
    "models['fithrf_glmdenoise'] = results_glmsingle['typec']['betasmd']\n",
    "models['fithrf_glmdenoise_rr'] = results_glmsingle['typed']['betasmd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get indices of repeated conditions to use for reliability calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare the results of different GLMs we are going to calculate the\n",
    "# voxel-wise split-half reliablity for each model. reliability values\n",
    "# reflect a correlation between beta weights for repeated presentations of\n",
    "# the same conditions. in short, we are going to check how\n",
    "# reliable/reproducible are the single trial responses to repeated\n",
    "# conditions estimated with each GLM type.\n",
    "\n",
    "# this NSD scan session has a large number of images that are just shown\n",
    "# once during the session, some images that are shown twice, and a few that\n",
    "# are shown three times. in the code below, we are attempting to locate the\n",
    "# indices in the beta weight GLMsingle outputs modelmd(x,y,z,trials) that\n",
    "# correspond to repeated images. here we only consider stimuli that have\n",
    "# been presented at least twice. for the purpose of the example we ignore\n",
    "# the 3rd repetition of the stimulus.\n",
    "\n",
    "# consolidate design matrices\n",
    "designALL = np.concatenate(design,axis=0)\n",
    "\n",
    "# construct a vector containing 0-indexed condition numbers in chronological order\n",
    "corder = []\n",
    "for p in range(designALL.shape[0]):\n",
    "    if np.any(designALL[p]):\n",
    "        corder.append(np.argwhere(designALL[p])[0,0])\n",
    "        \n",
    "corder = np.array(corder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the first few entries\n",
    "print(corder[:3])\n",
    "\n",
    "# note that [374 496 7] means that the first stimulus trial involved\n",
    "# presentation of the 374th condition (zero-indexed), the second stimulus trial \n",
    "# involved presentation of the 496th condition, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to compute split-half reliability, we have to do some indexing.\n",
    "# we want to find images with least two repetitions and then prepare a\n",
    "# useful matrix of indices that refer to when these occur.\n",
    "\n",
    "repindices = [] # 2 x images containing stimulus trial indices.\n",
    "\n",
    "# the first row refers to the first presentation; the second row refers to\n",
    "# the second presentation.\n",
    "for p in range(designALL.shape[1]): # loop over every condition\n",
    "    \n",
    "    temp = np.argwhere(corder==p)[:,0] # find indices where this condition was shown\n",
    "    \n",
    "    # note that for conditions with 3 presentations, we are simply ignoring the third trial\n",
    "    if len(temp) >= 2:\n",
    "        repindices.append([temp[0], temp[1]]) \n",
    "\n",
    "repindices = np.vstack(np.array(repindices)).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at a few entries\n",
    "print(repindices[:,:3])\n",
    "\n",
    "# notice that the first condition is presented on the 216th zero-indexed \n",
    "# stimulus trial and the 485th stimulus trial, the second condition is presented on the\n",
    "# 217th and 620st stimulus trials, and so on.\n",
    "\n",
    "print(f'there are {repindices.shape[1]} repeated conditions in the experiment')\n",
    "\n",
    "# now, for each voxel we are going to correlate beta weights describing the\n",
    "# response to images presented for the first time with beta weights\n",
    "# describing the response from the repetition of the same image. with 136\n",
    "# repeated conditions, the correlation for each voxel will reflect the\n",
    "# relationship between two vectors with 136 beta weights each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize cortical ROI defining visually-responsive areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask defining liberal visual cortex ROI. \"nsdgeneral\" is a general ROI \n",
    "# that was manually drawn on fsaverage covering voxels responsive to the NSD experiment \n",
    "# in the posterior aspect of cortex. for the sake of simplicity we will focus \n",
    "# on voxels within this ROI in computing split-half reliability\n",
    "\n",
    "nsdgeneral_roi = roi.astype(float)\n",
    "\n",
    "# convert voxels outside ROI to nan for overlay plotting\n",
    "nsdgeneral_roi[nsdgeneral_roi==0] = np.nan \n",
    "\n",
    "# get mean fMRI volume from run 1\n",
    "meanvol = np.squeeze(np.mean(data[0].reshape(xyzt),axis=3))\n",
    "\n",
    "# plot ROI on top of overlay\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(meanvol,cmap='gray')\n",
    "plt.imshow(nsdgeneral_roi,cmap='Blues',clim=(0,2))\n",
    "\n",
    "plt.title('voxels in nsdgeneral ROI')\n",
    "plt.box(False)\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute median split-half reliability within the ROI for each beta version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, let's compute split-half reliability. we are going to loop\n",
    "# through our 4 models and calculate split-half reliability for each of them\n",
    "\n",
    "vox_reliabilities = [] # output variable for reliability values\n",
    "\n",
    "modelnames = list(models.keys())\n",
    "\n",
    "# for each beta version...\n",
    "for m in range(len(modelnames)):\n",
    "    \n",
    "    print(f'computing reliability for beta version: {modelnames[m]}')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # get the repeated-condition GLM betas using our repindices variable\n",
    "    betas = models[modelnames[m]][:,:,:,repindices] # automatically reshapes to (X x Y x Z x 2 x nConditions)\n",
    "    x,y,z = betas.shape[:3] \n",
    "    \n",
    "    rels = np.full((x,y,z),np.nan)\n",
    "    \n",
    "    # loop through voxels in the 3D volume...\n",
    "    for xx in tqdm(range(x)):\n",
    "        for yy in range(y):\n",
    "            for zz in range(z):\n",
    "                \n",
    "                # reliability at a given voxel is pearson correlation between response profiles from first and \n",
    "                # second image presentations (dim = 136 conditions)\n",
    "                rels[xx,yy,zz] = np.corrcoef(betas[xx,yy,zz,0],\n",
    "                                             betas[xx,yy,zz,1])[1,0]\n",
    "          \n",
    "    vox_reliabilities.append(rels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess change in reliability yielded by GLMsingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each GLM we will calculate median reliability for voxels within the\n",
    "# nsdgeneral visual ROI and compare using a bar graph\n",
    "\n",
    "comparison = []\n",
    "for vr in vox_reliabilities:\n",
    "    comparison.append(np.nanmedian(vr[nsdgeneral_roi==1]))\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(121)\n",
    "plt.bar(np.arange(len(comparison)),comparison,width=0.5)\n",
    "plt.title('Median voxel split-half reliability of GLM models')\n",
    "plt.xticks(np.arange(4),np.array(['ASSUMEHRF', 'FITHRF', 'FITHRF\\nGLMDENOISE', 'FITHRF\\nGLMDENOISE\\nRR']));\n",
    "plt.ylim([0.1,0.2])\n",
    "\n",
    "# draw plot showing the change in reliability between the baseline GLM\n",
    "# and the final output of GLMsingle (fithrf-glmdenoise-RR betas)\n",
    "vox_improvement = np.squeeze(vox_reliabilities[3] - vox_reliabilities[0])\n",
    "vox_improvement[nsdgeneral_roi != 1] = np.nan\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(meanvol,cmap='gray',aspect='auto')\n",
    "plt.imshow(vox_improvement,cmap='RdBu_r',clim=(-0.3,0.3),aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('change in nsdgeneral voxel reliability**\\ndue to GLMsingle (r)')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('\\n**plotting (FITHRF_GLMDENOISE_RR - ASSUMEHRF) reliabilities');\n",
    "\n",
    "# notice that there is systematic increase in reliability moving from the\n",
    "# first to the second to the third to the final fourth version of the GLM\n",
    "# results. these increases reflect, respectively, the addition of HRF\n",
    "# fitting, the derivation and use of data-driven nuisance regressors, and\n",
    "# the use of ridge regression as a way to regularize the instability of\n",
    "# closely spaced experimental trials. depending on one's experimental\n",
    "# goals, it is possible with setting of option flags to activate a subset\n",
    "# of these analysis features.\n",
    "\n",
    "# also, keep in mind that in the above figure, we are simply showing the\n",
    "# median as a metric of the central tendency (you may want to peruse\n",
    "# individual voxels in scatter plots, for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
